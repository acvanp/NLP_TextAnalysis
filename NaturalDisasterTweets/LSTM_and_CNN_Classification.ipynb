{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_uuid": "abb7e3c30b8a412a50c6b451c49939e3cf4bc11b",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "import random\n",
    "import copy\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "import re\n",
    "import torch\n",
    "\n",
    "#import spacy\n",
    "from tqdm import tqdm_notebook, tnrange\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "tqdm.pandas(desc='Progress')\n",
    "from collections import Counter\n",
    "\n",
    "from nltk import word_tokenize\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "from torch.autograd import Variable\n",
    "from sklearn.metrics import f1_score\n",
    "import os \n",
    "\n",
    "#from keras.preprocessing.text import Tokenizer\n",
    "#from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# cross validation and metrics\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import f1_score\n",
    "from torch.optim.optimizer import Optimizer\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from multiprocessing import  Pool\n",
    "from functools import partial\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "import torch as t\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu116\n",
    "#%pip install -U scikit-learn scipy matplotlib\n",
    "#%pip install tqdm\n",
    "#%pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "gather": {
     "logged": 1667241225826
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xlrd in c:\\users\\owner\\anaconda3\\lib\\site-packages (2.0.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: openpyxl in c:\\users\\owner\\anaconda3\\lib\\site-packages (3.0.9)\n",
      "Requirement already satisfied: et-xmlfile in c:\\users\\owner\\anaconda3\\lib\\site-packages (from openpyxl) (1.1.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install xlrd\n",
    "%pip install openpyxl\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.svm import LinearSVC\n",
    "#from sklearn import svm \n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from nltk.corpus import stopwords\n",
    "import time\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import random\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn import metrics\n",
    "from datetime import datetime\n",
    "from ast import literal_eval\n",
    "#from torchtext import data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('wordnet')\n",
    "#nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "gather": {
     "logged": 1667241274822
    }
   },
   "outputs": [],
   "source": [
    "# load the pre-processed train and test data\n",
    "# LoadData...ipynb should have been used as a first step to process the text\n",
    "\n",
    "df_train = pd.read_csv('df_train_shuffle_processed.csv')\n",
    "df_train = df_train.sample(frac=1)# shuffle\n",
    "test_all = df_train.iloc[-200:, :]# set aside the test data\n",
    "#df_train = df_train.iloc[:-200, :]# split off the train from the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5846    Ashley and I on going to hurricane harbor Frid...\n",
       "632     Dr. Bengston on #wildfire management: ÛÏnumbe...\n",
       "7144    Govt allocating 1.3 bn for flood action: Issue...\n",
       "1024    And please don't flood poor @RobertBEnglund's ...\n",
       "3230    It's cold and my head wants to explode.. The j...\n",
       "                              ...                        \n",
       "5969    'Nobody remembers who came in second.' Charles...\n",
       "7548    @Michael5SOS FUCKING LIVE HERE IM SURPRISED IV...\n",
       "1232    Busty blonde teen Natalia Starr fucks the secu...\n",
       "1219    Malaysia seem more certain than France.\\n\\nPla...\n",
       "6283    I will never support looting or the burning of...\n",
       "Name: text, Length: 7613, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#%pip install gensim\n",
    "df_train.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19904\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'co': 0,\n",
       " 'http': 1,\n",
       " 'get': 2,\n",
       " 'https': 3,\n",
       " 'like': 4,\n",
       " 'fire': 5,\n",
       " 'û_': 6,\n",
       " 'amp': 7,\n",
       " 'go': 8,\n",
       " 'bomb': 9,\n",
       " 'new': 10,\n",
       " 'via': 11,\n",
       " '2': 12,\n",
       " 'people': 13,\n",
       " 'say': 14,\n",
       " 'one': 15,\n",
       " 'news': 16,\n",
       " 'burn': 17,\n",
       " 'kill': 18,\n",
       " 'make': 19,\n",
       " 'time': 20,\n",
       " 'flood': 21,\n",
       " 'video': 22,\n",
       " 'crash': 23,\n",
       " 'come': 24,\n",
       " 'disaster': 25,\n",
       " 'emergency': 26,\n",
       " 'build': 27,\n",
       " 'body': 28,\n",
       " 'attack': 29,\n",
       " 'see': 30,\n",
       " 'look': 31,\n",
       " 'home': 32,\n",
       " 'police': 33,\n",
       " 'take': 34,\n",
       " 'know': 35,\n",
       " 'think': 36,\n",
       " 'would': 37,\n",
       " 'u': 38,\n",
       " 'love': 39,\n",
       " '3': 40,\n",
       " 'still': 41,\n",
       " 'train': 42,\n",
       " 'storm': 43,\n",
       " 'back': 44,\n",
       " 'us': 45,\n",
       " 'california': 46,\n",
       " 'bag': 47,\n",
       " 'suicide': 48,\n",
       " 'want': 49,\n",
       " 'collapse': 50,\n",
       " 'watch': 51,\n",
       " 'live': 52,\n",
       " 'day': 53,\n",
       " 'man': 54,\n",
       " 'scream': 55,\n",
       " '1': 56,\n",
       " 'work': 57,\n",
       " 'first': 58,\n",
       " 'rt': 59,\n",
       " 'cause': 60,\n",
       " 'let': 61,\n",
       " 'world': 62,\n",
       " 'nuclear': 63,\n",
       " 'two': 64,\n",
       " 'need': 65,\n",
       " 'war': 66,\n",
       " 'drown': 67,\n",
       " 'today': 68,\n",
       " 'û': 69,\n",
       " 'wreck': 70,\n",
       " 'youtube': 71,\n",
       " 'year': 72,\n",
       " 'dead': 73,\n",
       " 'plan': 74,\n",
       " 'destroy': 75,\n",
       " '5': 76,\n",
       " 'gt': 77,\n",
       " '4': 78,\n",
       " 'full': 79,\n",
       " 'car': 80,\n",
       " 'feel': 81,\n",
       " 'old': 82,\n",
       " 'hiroshima': 83,\n",
       " 'fuck': 84,\n",
       " 'help': 85,\n",
       " 'life': 86,\n",
       " 'accident': 87,\n",
       " 'fear': 88,\n",
       " 'families': 89,\n",
       " 'good': 90,\n",
       " 'last': 91,\n",
       " 'may': 92,\n",
       " 'even': 93,\n",
       " 'ûªs': 94,\n",
       " 'run': 95,\n",
       " 'sink': 96,\n",
       " 'blow': 97,\n",
       " 'many': 98,\n",
       " 'rescue': 99,\n",
       " '2015': 100,\n",
       " 'find': 101,\n",
       " 'service': 102,\n",
       " 'use': 103,\n",
       " 'could': 104,\n",
       " 'wound': 105,\n",
       " 'die': 106,\n",
       " 'break': 107,\n",
       " 'way': 108,\n",
       " 'survive': 109,\n",
       " 'report': 110,\n",
       " 'years': 111,\n",
       " 'w': 112,\n",
       " 'leave': 113,\n",
       " 'evacuate': 114,\n",
       " 'explode': 115,\n",
       " 'riot': 116,\n",
       " 'call': 117,\n",
       " 'save': 118,\n",
       " 'wave': 119,\n",
       " 'panic': 120,\n",
       " 'collide': 121,\n",
       " 'death': 122,\n",
       " 'derail': 123,\n",
       " 'fall': 124,\n",
       " 'right': 125,\n",
       " 'mass': 126,\n",
       " 'read': 127,\n",
       " 'damage': 128,\n",
       " 'please': 129,\n",
       " 'quarantine': 130,\n",
       " 'best': 131,\n",
       " 'mh370': 132,\n",
       " 'army': 133,\n",
       " 'another': 134,\n",
       " 'house': 135,\n",
       " 'wildfire': 136,\n",
       " 'really': 137,\n",
       " 'hear': 138,\n",
       " 'school': 139,\n",
       " 'crush': 140,\n",
       " 'water': 141,\n",
       " 'lol': 142,\n",
       " 'stop': 143,\n",
       " 'black': 144,\n",
       " 'miss': 145,\n",
       " 'warn': 146,\n",
       " 'hot': 147,\n",
       " 'forest': 148,\n",
       " 'fatal': 149,\n",
       " 'detonate': 150,\n",
       " 'obama': 151,\n",
       " 'deluge': 152,\n",
       " '11': 153,\n",
       " 'hit': 154,\n",
       " 'pm': 155,\n",
       " 'much': 156,\n",
       " 'reddit': 157,\n",
       " '8': 158,\n",
       " 'northern': 159,\n",
       " 'state': 160,\n",
       " 'demolish': 161,\n",
       " 'thank': 162,\n",
       " 'electrocute': 163,\n",
       " 'try': 164,\n",
       " 'never': 165,\n",
       " 'cross': 166,\n",
       " 'set': 167,\n",
       " 'city': 168,\n",
       " 'confirm': 169,\n",
       " 'play': 170,\n",
       " '9': 171,\n",
       " 'head': 172,\n",
       " 'great': 173,\n",
       " 'legionnaires': 174,\n",
       " 'change': 175,\n",
       " 'flame': 176,\n",
       " 'bomber': 177,\n",
       " 'japan': 178,\n",
       " 'hijack': 179,\n",
       " 'shoot': 180,\n",
       " 'end': 181,\n",
       " 'start': 182,\n",
       " 'latest': 183,\n",
       " 'obliterate': 184,\n",
       " 'every': 185,\n",
       " 'rain': 186,\n",
       " 'wind': 187,\n",
       " 'release': 188,\n",
       " 'typhoon': 189,\n",
       " 'show': 190,\n",
       " 'injure': 191,\n",
       " 'atomic': 192,\n",
       " 'top': 193,\n",
       " 'issue': 194,\n",
       " 'near': 195,\n",
       " 'everyone': 196,\n",
       " '6': 197,\n",
       " 'give': 198,\n",
       " 'shit': 199,\n",
       " 'ever': 200,\n",
       " 'keep': 201,\n",
       " 'oil': 202,\n",
       " 'post': 203,\n",
       " 'land': 204,\n",
       " 'truck': 205,\n",
       " 'update': 206,\n",
       " 'sign': 207,\n",
       " 'happen': 208,\n",
       " 'blaze': 209,\n",
       " 'win': 210,\n",
       " 'god': 211,\n",
       " 'smoke': 212,\n",
       " 'content': 213,\n",
       " 'face': 214,\n",
       " 'im': 215,\n",
       " 'charge': 216,\n",
       " 'since': 217,\n",
       " 'game': 218,\n",
       " 'hope': 219,\n",
       " 'send': 220,\n",
       " '15': 221,\n",
       " 'weather': 222,\n",
       " 'military': 223,\n",
       " 'tell': 224,\n",
       " 'ass': 225,\n",
       " '10': 226,\n",
       " 'pick': 227,\n",
       " 'without': 228,\n",
       " 'earthquake': 229,\n",
       " 'malaysia': 230,\n",
       " 'debris': 231,\n",
       " 'night': 232,\n",
       " 'murder': 233,\n",
       " 'next': 234,\n",
       " 'evacuation': 235,\n",
       " 'wild': 236,\n",
       " 'check': 237,\n",
       " 'sound': 238,\n",
       " 'well': 239,\n",
       " 'suspect': 240,\n",
       " 'light': 241,\n",
       " 'ûª': 242,\n",
       " '08': 243,\n",
       " 'area': 244,\n",
       " 'drive': 245,\n",
       " 'heat': 246,\n",
       " 'little': 247,\n",
       " 'thunderstorm': 248,\n",
       " 'bus': 249,\n",
       " 'fight': 250,\n",
       " 'guy': 251,\n",
       " 'food': 252,\n",
       " 'trap': 253,\n",
       " 'job': 254,\n",
       " 'spill': 255,\n",
       " 'move': 256,\n",
       " 'severe': 257,\n",
       " 'movie': 258,\n",
       " 'bad': 259,\n",
       " 'high': 260,\n",
       " 'devastate': 261,\n",
       " 'thunder': 262,\n",
       " 'always': 263,\n",
       " 'explosion': 264,\n",
       " '70': 265,\n",
       " '7': 266,\n",
       " 'fan': 267,\n",
       " 'ruin': 268,\n",
       " 'bloody': 269,\n",
       " 'lightning': 270,\n",
       " 'free': 271,\n",
       " 'natural': 272,\n",
       " 'lead': 273,\n",
       " 'weapon': 274,\n",
       " 'battle': 275,\n",
       " 'kid': 276,\n",
       " 'injuries': 277,\n",
       " 'red': 278,\n",
       " 'hail': 279,\n",
       " 'family': 280,\n",
       " 'refugees': 281,\n",
       " 'weapons': 282,\n",
       " 'blood': 283,\n",
       " 'gonna': 284,\n",
       " 'boy': 285,\n",
       " 'also': 286,\n",
       " 'loud': 287,\n",
       " 'tonight': 288,\n",
       " 'someone': 289,\n",
       " 'long': 290,\n",
       " 'p': 291,\n",
       " 'lot': 292,\n",
       " 'put': 293,\n",
       " 'lose': 294,\n",
       " 'summer': 295,\n",
       " 'displace': 296,\n",
       " 'baby': 297,\n",
       " 'fatalities': 298,\n",
       " 'market': 299,\n",
       " 'failure': 300,\n",
       " 'sinkhole': 301,\n",
       " 'china': 302,\n",
       " 'air': 303,\n",
       " 'photo': 304,\n",
       " 'injury': 305,\n",
       " 'bridge': 306,\n",
       " 'hazard': 307,\n",
       " 'rise': 308,\n",
       " 'harm': 309,\n",
       " 'ûò': 310,\n",
       " 'eye': 311,\n",
       " 'migrants': 312,\n",
       " 'big': 313,\n",
       " 'whole': 314,\n",
       " 'turn': 315,\n",
       " 'follow': 316,\n",
       " 'hurricane': 317,\n",
       " 'collision': 318,\n",
       " 'order': 319,\n",
       " 'wreckage': 320,\n",
       " 'outbreak': 321,\n",
       " 'wake': 322,\n",
       " 'affect': 323,\n",
       " 'deaths': 324,\n",
       " 'terrorist': 325,\n",
       " '30': 326,\n",
       " 'saudi': 327,\n",
       " 'destruction': 328,\n",
       " 'phone': 329,\n",
       " '40': 330,\n",
       " 'part': 331,\n",
       " 'remember': 332,\n",
       " 'ambulance': 333,\n",
       " 'bring': 334,\n",
       " 'derailment': 335,\n",
       " '05': 336,\n",
       " 'week': 337,\n",
       " 'road': 338,\n",
       " 'close': 339,\n",
       " 'trauma': 340,\n",
       " 'real': 341,\n",
       " 'rescuers': 342,\n",
       " 'around': 343,\n",
       " 'drought': 344,\n",
       " 'wait': 345,\n",
       " 'twister': 346,\n",
       " 'word': 347,\n",
       " 'windstorm': 348,\n",
       " 'survivors': 349,\n",
       " 'county': 350,\n",
       " 'story': 351,\n",
       " 'white': 352,\n",
       " 'officer': 353,\n",
       " 'island': 354,\n",
       " 'boat': 355,\n",
       " 'catastrophe': 356,\n",
       " 'landslide': 357,\n",
       " 'self': 358,\n",
       " 'ship': 359,\n",
       " 'dust': 360,\n",
       " 'girl': 361,\n",
       " 'whirlwind': 362,\n",
       " 'sandstorm': 363,\n",
       " 'open': 364,\n",
       " 'twitter': 365,\n",
       " 'ok': 366,\n",
       " 'danger': 367,\n",
       " 'violent': 368,\n",
       " 'women': 369,\n",
       " 'investigators': 370,\n",
       " 'half': 371,\n",
       " 'armageddon': 372,\n",
       " 'structural': 373,\n",
       " 'stock': 374,\n",
       " 'bleed': 375,\n",
       " 'group': 376,\n",
       " 'woman': 377,\n",
       " 'hazardous': 378,\n",
       " 'things': 379,\n",
       " '0': 380,\n",
       " 'devastation': 381,\n",
       " 'mudslide': 382,\n",
       " 'engulf': 383,\n",
       " 'effect': 384,\n",
       " 'thing': 385,\n",
       " 'hostage': 386,\n",
       " 'famine': 387,\n",
       " 'iran': 388,\n",
       " 'airplane': 389,\n",
       " 'hold': 390,\n",
       " 'wanna': 391,\n",
       " 'blast': 392,\n",
       " 'search': 393,\n",
       " 'tragedy': 394,\n",
       " 'link': 395,\n",
       " 'away': 396,\n",
       " 'b': 397,\n",
       " 'curfew': 398,\n",
       " 'second': 399,\n",
       " 'power': 400,\n",
       " 'least': 401,\n",
       " 'trouble': 402,\n",
       " 'traumatise': 403,\n",
       " 'bang': 404,\n",
       " 'saw': 405,\n",
       " 'hostages': 406,\n",
       " 'horrible': 407,\n",
       " 'drink': 408,\n",
       " 'past': 409,\n",
       " 'better': 410,\n",
       " 'hundreds': 411,\n",
       " 'zone': 412,\n",
       " 'mean': 413,\n",
       " 'chemical': 414,\n",
       " 'anniversary': 415,\n",
       " 'tomorrow': 416,\n",
       " 'august': 417,\n",
       " 'deal': 418,\n",
       " 'cliff': 419,\n",
       " 'inundate': 420,\n",
       " 'mosque': 421,\n",
       " 'stand': 422,\n",
       " 'place': 423,\n",
       " 'heart': 424,\n",
       " 'national': 425,\n",
       " 'flatten': 426,\n",
       " 'lava': 427,\n",
       " 'cool': 428,\n",
       " 'must': 429,\n",
       " 'raze': 430,\n",
       " 'n': 431,\n",
       " 'book': 432,\n",
       " 'oh': 433,\n",
       " 'care': 434,\n",
       " 'pandemonium': 435,\n",
       " 'hijacker': 436,\n",
       " 'stay': 437,\n",
       " 'desolation': 438,\n",
       " 'catastrophic': 439,\n",
       " 'casualties': 440,\n",
       " 'talk': 441,\n",
       " 'ban': 442,\n",
       " 'fedex': 443,\n",
       " 'terrorism': 444,\n",
       " 'bioterror': 445,\n",
       " 'shoulder': 446,\n",
       " 'ebay': 447,\n",
       " 'project': 448,\n",
       " 'sit': 449,\n",
       " 'murderer': 450,\n",
       " 'massacre': 451,\n",
       " 'annihilate': 452,\n",
       " 'catch': 453,\n",
       " 'reunion': 454,\n",
       " 'expect': 455,\n",
       " 'become': 456,\n",
       " 'tornado': 457,\n",
       " 'swallow': 458,\n",
       " 'meltdown': 459,\n",
       " 'soon': 460,\n",
       " '00': 461,\n",
       " 'calgary': 462,\n",
       " 'star': 463,\n",
       " 'demolition': 464,\n",
       " 'possible': 465,\n",
       " 'hat': 466,\n",
       " 'believe': 467,\n",
       " 'plane': 468,\n",
       " 'case': 469,\n",
       " 'river': 470,\n",
       " 'something': 471,\n",
       " 'south': 472,\n",
       " 'song': 473,\n",
       " 'apocalypse': 474,\n",
       " 'fatality': 475,\n",
       " 'begin': 476,\n",
       " 'minute': 477,\n",
       " 'sure': 478,\n",
       " 'declare': 479,\n",
       " 'men': 480,\n",
       " 'casualty': 481,\n",
       " 'support': 482,\n",
       " 'government': 483,\n",
       " 'detonation': 484,\n",
       " 'strike': 485,\n",
       " 'e': 486,\n",
       " 'tsunami': 487,\n",
       " 'security': 488,\n",
       " 'pkk': 489,\n",
       " 'walk': 490,\n",
       " 'eyewitness': 491,\n",
       " 'traffic': 492,\n",
       " 'three': 493,\n",
       " 'india': 494,\n",
       " 'longer': 495,\n",
       " 'reason': 496,\n",
       " 'line': 497,\n",
       " 'st': 498,\n",
       " 'volcano': 499,\n",
       " 'gun': 500,\n",
       " 'arson': 501,\n",
       " 'share': 502,\n",
       " 'level': 503,\n",
       " 'beautiful': 504,\n",
       " 'due': 505,\n",
       " 'bush': 506,\n",
       " 'ur': 507,\n",
       " 'sue': 508,\n",
       " 'prebreak': 509,\n",
       " 'c': 510,\n",
       " 'dog': 511,\n",
       " 'responders': 512,\n",
       " 'listen': 513,\n",
       " 'name': 514,\n",
       " 'airport': 515,\n",
       " 'r': 516,\n",
       " 'cyclone': 517,\n",
       " 'yet': 518,\n",
       " 'already': 519,\n",
       " 'avalanche': 520,\n",
       " 'fun': 521,\n",
       " '16': 522,\n",
       " 'obliteration': 523,\n",
       " '50': 524,\n",
       " 'learn': 525,\n",
       " 'block': 526,\n",
       " 'lt': 527,\n",
       " 'upheaval': 528,\n",
       " 'days': 529,\n",
       " 'rainstorm': 530,\n",
       " 'hours': 531,\n",
       " 'force': 532,\n",
       " 'nothing': 533,\n",
       " 'media': 534,\n",
       " 'hellfire': 535,\n",
       " 'ablaze': 536,\n",
       " 'officials': 537,\n",
       " 'far': 538,\n",
       " 'person': 539,\n",
       " 'ask': 540,\n",
       " 'image': 541,\n",
       " 'seismic': 542,\n",
       " '16yr': 543,\n",
       " 'team': 544,\n",
       " 'ûó': 545,\n",
       " 'actually': 546,\n",
       " 'side': 547,\n",
       " 'israeli': 548,\n",
       " 'inside': 549,\n",
       " 'bar': 550,\n",
       " 'turkey': 551,\n",
       " 'peace': 552,\n",
       " 'sirens': 553,\n",
       " 'policy': 554,\n",
       " 'snowstorm': 555,\n",
       " 'music': 556,\n",
       " 'ûªt': 557,\n",
       " 'america': 558,\n",
       " 'action': 559,\n",
       " 'crew': 560,\n",
       " 'nearby': 561,\n",
       " 'reactor': 562,\n",
       " 'rule': 563,\n",
       " 'add': 564,\n",
       " 'bioterrorism': 565,\n",
       " 'pic': 566,\n",
       " 'yes': 567,\n",
       " '25': 568,\n",
       " 'point': 569,\n",
       " 'brown': 570,\n",
       " 'abc': 571,\n",
       " 'ago': 572,\n",
       " 'lab': 573,\n",
       " 'buy': 574,\n",
       " '06': 575,\n",
       " 'site': 576,\n",
       " 'health': 577,\n",
       " 'transport': 578,\n",
       " 'north': 579,\n",
       " 'hand': 580,\n",
       " 'horror': 581,\n",
       " 'soudelor': 582,\n",
       " 'lie': 583,\n",
       " 'blight': 584,\n",
       " 'anything': 585,\n",
       " 'mark': 586,\n",
       " 'pay': 587,\n",
       " 'mp': 588,\n",
       " 'tweet': 589,\n",
       " 'offensive': 590,\n",
       " 'stretcher': 591,\n",
       " 'grow': 592,\n",
       " 'nowplaying': 593,\n",
       " 'siren': 594,\n",
       " 'west': 595,\n",
       " 'isis': 596,\n",
       " 'low': 597,\n",
       " 'rubble': 598,\n",
       " 'history': 599,\n",
       " 'cover': 600,\n",
       " 'control': 601,\n",
       " '01': 602,\n",
       " 'tree': 603,\n",
       " 'american': 604,\n",
       " 'bc': 605,\n",
       " 'hell': 606,\n",
       " 'x': 607,\n",
       " 'desolate': 608,\n",
       " 'photos': 609,\n",
       " 'business': 610,\n",
       " 'center': 611,\n",
       " 'yeah': 612,\n",
       " 'hey': 613,\n",
       " 'aircraft': 614,\n",
       " '\\n': 615,\n",
       " 'alarm': 616,\n",
       " 'children': 617,\n",
       " 'almost': 618,\n",
       " 'helicopter': 619,\n",
       " 'outside': 620,\n",
       " 'saipan': 621,\n",
       " 'spot': 622,\n",
       " 'flag': 623,\n",
       " 'bigger': 624,\n",
       " 'conclusively': 625,\n",
       " 'knock': 626,\n",
       " 'data': 627,\n",
       " 'pakistan': 628,\n",
       " 'money': 629,\n",
       " 'prepare': 630,\n",
       " 'literally': 631,\n",
       " 'memories': 632,\n",
       " 'street': 633,\n",
       " '20': 634,\n",
       " 'maybe': 635,\n",
       " 'claim': 636,\n",
       " 'country': 637,\n",
       " 'allow': 638,\n",
       " 'act': 639,\n",
       " 'rock': 640,\n",
       " 'reuters': 641,\n",
       " 'cake': 642,\n",
       " 'course': 643,\n",
       " 'bestnaijamade': 644,\n",
       " 'anyone': 645,\n",
       " 'tv': 646,\n",
       " 'couple': 647,\n",
       " 'damn': 648,\n",
       " 'manslaughter': 649,\n",
       " 'pretty': 650,\n",
       " 'interest': 651,\n",
       " 'coach': 652,\n",
       " 'might': 653,\n",
       " 'park': 654,\n",
       " 'hailstorm': 655,\n",
       " 'track': 656,\n",
       " 'wonder': 657,\n",
       " 'everything': 658,\n",
       " 'seek': 659,\n",
       " 'ball': 660,\n",
       " 'cop': 661,\n",
       " 'aug': 662,\n",
       " 'trench': 663,\n",
       " 'amid': 664,\n",
       " 'hollywood': 665,\n",
       " 'camp': 666,\n",
       " 'pass': 667,\n",
       " 'rd': 668,\n",
       " 'though': 669,\n",
       " 'online': 670,\n",
       " 'probably': 671,\n",
       " 'fast': 672,\n",
       " 'finally': 673,\n",
       " 'happy': 674,\n",
       " 'property': 675,\n",
       " 'town': 676,\n",
       " 'space': 677,\n",
       " 'child': 678,\n",
       " 'scar': 679,\n",
       " 'mom': 680,\n",
       " 'worry': 681,\n",
       " '12': 682,\n",
       " 'sorry': 683,\n",
       " 'miners': 684,\n",
       " 'australia': 685,\n",
       " 'class': 686,\n",
       " 'crisis': 687,\n",
       " 'blue': 688,\n",
       " 'date': 689,\n",
       " 'refugio': 690,\n",
       " 'costlier': 691,\n",
       " 'nearly': 692,\n",
       " 'sensor': 693,\n",
       " 'chance': 694,\n",
       " 'leather': 695,\n",
       " '17': 696,\n",
       " 'okay': 697,\n",
       " 'hate': 698,\n",
       " 'number': 699,\n",
       " 'la': 700,\n",
       " 'morning': 701,\n",
       " 'meet': 702,\n",
       " 'annihilation': 703,\n",
       " 'ca': 704,\n",
       " 'total': 705,\n",
       " 'huge': 706,\n",
       " 'omg': 707,\n",
       " 'escape': 708,\n",
       " 'result': 709,\n",
       " 'trust': 710,\n",
       " 'unite': 711,\n",
       " '60': 712,\n",
       " 'wrong': 713,\n",
       " 'fukushima': 714,\n",
       " 'avoid': 715,\n",
       " 'vehicle': 716,\n",
       " 'worst': 717,\n",
       " 'cars': 718,\n",
       " 'youth': 719,\n",
       " 'east': 720,\n",
       " 'russian': 721,\n",
       " '13': 722,\n",
       " 'continue': 723,\n",
       " 'giant': 724,\n",
       " 'film': 725,\n",
       " 'texas': 726,\n",
       " 'crazy': 727,\n",
       " 'major': 728,\n",
       " 'daily': 729,\n",
       " 'company': 730,\n",
       " 'usa': 731,\n",
       " 'beach': 732,\n",
       " 'agree': 733,\n",
       " 'write': 734,\n",
       " 'gbbo': 735,\n",
       " 'capture': 736,\n",
       " 'drake': 737,\n",
       " 'meek': 738,\n",
       " 'view': 739,\n",
       " 'computers': 740,\n",
       " 'official': 741,\n",
       " 'carry': 742,\n",
       " 'ignition': 743,\n",
       " 'flash': 744,\n",
       " 'heavy': 745,\n",
       " 'lord': 746,\n",
       " 'test': 747,\n",
       " 'angry': 748,\n",
       " 'wow': 749,\n",
       " 'mayhem': 750,\n",
       " 'comment': 751,\n",
       " 'across': 752,\n",
       " 'others': 753,\n",
       " 'hard': 754,\n",
       " 'entire': 755,\n",
       " 'cry': 756,\n",
       " 'appear': 757,\n",
       " 'mount': 758,\n",
       " 'anthrax': 759,\n",
       " 'toddler': 760,\n",
       " 'radio': 761,\n",
       " 'haha': 762,\n",
       " 'record': 763,\n",
       " 'sky': 764,\n",
       " 'dont': 765,\n",
       " 'bite': 766,\n",
       " 'create': 767,\n",
       " 'pilot': 768,\n",
       " 'pain': 769,\n",
       " 'public': 770,\n",
       " 'horse': 771,\n",
       " 'village': 772,\n",
       " 'temple': 773,\n",
       " 'flight': 774,\n",
       " 'muslims': 775,\n",
       " 'disea': 776,\n",
       " 'declaration': 777,\n",
       " 'centre': 778,\n",
       " 'cut': 779,\n",
       " 'totally': 780,\n",
       " 'blizzard': 781,\n",
       " 'beat': 782,\n",
       " 'uk': 783,\n",
       " 'target': 784,\n",
       " 'mishaps': 785,\n",
       " 'large': 786,\n",
       " 'islam': 787,\n",
       " '24': 788,\n",
       " 'poor': 789,\n",
       " 'emmerdale': 790,\n",
       " 'chicago': 791,\n",
       " 'exchange': 792,\n",
       " '100': 793,\n",
       " 'else': 794,\n",
       " 'christian': 795,\n",
       " 'quiz': 796,\n",
       " 'ladies': 797,\n",
       " 'wed': 798,\n",
       " 'myanmar': 799,\n",
       " 'friends': 800,\n",
       " 'green': 801,\n",
       " 'mad': 802,\n",
       " 'arrest': 803,\n",
       " '18': 804,\n",
       " 'aftershock': 805,\n",
       " 'forget': 806,\n",
       " 'cost': 807,\n",
       " 'room': 808,\n",
       " 'question': 809,\n",
       " 'insurance': 810,\n",
       " 'downtown': 811,\n",
       " 'spring': 812,\n",
       " 'vote': 813,\n",
       " 'human': 814,\n",
       " 'hire': 815,\n",
       " 'british': 816,\n",
       " 'front': 817,\n",
       " 'mph': 818,\n",
       " 'global': 819,\n",
       " 'favorite': 820,\n",
       " 'occur': 821,\n",
       " 'info': 822,\n",
       " 'arsonist': 823,\n",
       " 'mine': 824,\n",
       " 'instead': 825,\n",
       " 'download': 826,\n",
       " 'travel': 827,\n",
       " 'return': 828,\n",
       " 'till': 829,\n",
       " 'piece': 830,\n",
       " 'alone': 831,\n",
       " 'thousands': 832,\n",
       " 'arrive': 833,\n",
       " 'climate': 834,\n",
       " 'chile': 835,\n",
       " 'friend': 836,\n",
       " 'pray': 837,\n",
       " 'ice': 838,\n",
       " 'virgin': 839,\n",
       " 'germs': 840,\n",
       " 'ready': 841,\n",
       " 'thursday': 842,\n",
       " 'nagasaki': 843,\n",
       " 'ûï': 844,\n",
       " 'vs': 845,\n",
       " 'deliver': 846,\n",
       " 'womens': 847,\n",
       " 'experts': 848,\n",
       " 'israel': 849,\n",
       " 'involve': 850,\n",
       " 'sleep': 851,\n",
       " 'risk': 852,\n",
       " 'pull': 853,\n",
       " 'investigate': 854,\n",
       " 'dance': 855,\n",
       " 'patience': 856,\n",
       " 'spend': 857,\n",
       " 'pile': 858,\n",
       " 'wall': 859,\n",
       " 'driver': 860,\n",
       " 'ppl': 861,\n",
       " 'sex': 862,\n",
       " 'behind': 863,\n",
       " 'guide': 864,\n",
       " 'true': 865,\n",
       " 'sea': 866,\n",
       " 'pradesh': 867,\n",
       " 'join': 868,\n",
       " 'york': 869,\n",
       " 'ancient': 870,\n",
       " 'bless': 871,\n",
       " 'disease': 872,\n",
       " 'outrage': 873,\n",
       " 'subreddits': 874,\n",
       " 'sad': 875,\n",
       " 'middle': 876,\n",
       " 'drop': 877,\n",
       " 'respond': 878,\n",
       " 'funtenna': 879,\n",
       " 'theater': 880,\n",
       " 'enjoy': 881,\n",
       " 'shift': 882,\n",
       " 'bed': 883,\n",
       " 'madhya': 884,\n",
       " 'moment': 885,\n",
       " 'russia': 886,\n",
       " 'season': 887,\n",
       " 'four': 888,\n",
       " 'former': 889,\n",
       " 'gop': 890,\n",
       " 'galactic': 891,\n",
       " 'wednesday': 892,\n",
       " 'party': 893,\n",
       " 'france': 894,\n",
       " 'gems': 895,\n",
       " 'nigerian': 896,\n",
       " 'young': 897,\n",
       " 'kick': 898,\n",
       " 'disney': 899,\n",
       " 'early': 900,\n",
       " 'series': 901,\n",
       " 'answer': 902,\n",
       " 'rly': 903,\n",
       " 'awesome': 904,\n",
       " 'king': 905,\n",
       " 'base': 906,\n",
       " 'gas': 907,\n",
       " 'soul': 908,\n",
       " 'mind': 909,\n",
       " 'potus': 910,\n",
       " 'biggest': 911,\n",
       " 'guess': 912,\n",
       " 'likely': 913,\n",
       " 'lmao': 914,\n",
       " 'pakistani': 915,\n",
       " 'nws': 916,\n",
       " 'cat': 917,\n",
       " 'scene': 918,\n",
       " 'dad': 919,\n",
       " 'steal': 920,\n",
       " 'ave': 921,\n",
       " 'bitch': 922,\n",
       " 'london': 923,\n",
       " 'niggas': 924,\n",
       " 'sick': 925,\n",
       " 'upon': 926,\n",
       " 'parole': 927,\n",
       " 'fashion': 928,\n",
       " 'colorado': 929,\n",
       " 'shots': 930,\n",
       " 'threaten': 931,\n",
       " 'list': 932,\n",
       " 'neighbour': 933,\n",
       " 'businesses': 934,\n",
       " 'unconfirmed': 935,\n",
       " 'enough': 936,\n",
       " 'lamp': 937,\n",
       " 'pamela': 938,\n",
       " 'shape': 939,\n",
       " 'career': 940,\n",
       " 'militants': 941,\n",
       " 'silver': 942,\n",
       " '70th': 943,\n",
       " 'desire': 944,\n",
       " 'concern': 945,\n",
       " 'bayelsa': 946,\n",
       " 'press': 947,\n",
       " 'v': 948,\n",
       " 'f': 949,\n",
       " 'mode': 950,\n",
       " 'match': 951,\n",
       " 'sport': 952,\n",
       " 'ahead': 953,\n",
       " 'washington': 954,\n",
       " 'understand': 955,\n",
       " 'mention': 956,\n",
       " 'ebola': 957,\n",
       " 'handbag': 958,\n",
       " 'cable': 959,\n",
       " 'petition': 960,\n",
       " 'worse': 961,\n",
       " 'review': 962,\n",
       " 'box': 963,\n",
       " 'art': 964,\n",
       " 'super': 965,\n",
       " 'deep': 966,\n",
       " 'apollo': 967,\n",
       " 'members': 968,\n",
       " 'hour': 969,\n",
       " 'throw': 970,\n",
       " 'cree': 971,\n",
       " 'safety': 972,\n",
       " 'eat': 973,\n",
       " '12000': 974,\n",
       " 'experience': 975,\n",
       " 'tote': 976,\n",
       " 'playlist': 977,\n",
       " 'article': 978,\n",
       " 'japanese': 979,\n",
       " 'ash': 980,\n",
       " 'geller': 981,\n",
       " 'single': 982,\n",
       " 'passengers': 983,\n",
       " 'cdt': 984,\n",
       " 'govt': 985,\n",
       " 'terror': 986,\n",
       " 'map': 987,\n",
       " 'include': 988,\n",
       " 'provoke': 989,\n",
       " 'freak': 990,\n",
       " 'department': 991,\n",
       " 'struggle': 992,\n",
       " 'court': 993,\n",
       " 'crematoria': 994,\n",
       " 'bear': 995,\n",
       " 'seem': 996,\n",
       " 'secret': 997,\n",
       " 'research': 998,\n",
       " 'account': 999,\n",
       " ...}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.test.utils import common_texts\n",
    "from gensim.models import Word2Vec\n",
    "import gensim.corpora\n",
    "\n",
    "gensim_model = Word2Vec.load(\"word2vec.model\")\n",
    "# load embeddings generated from Gensim script in separate notebook\n",
    "myembeddings = {i:j for i,j in zip(gensim_model.wv.index_to_key, range(len(gensim_model.wv.index_to_key)))}\n",
    "print(len(myembeddings))\n",
    "myembeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "gather": {
     "logged": 1667241275276
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200, 7)\n",
      "(7613, 7)\n"
     ]
    }
   ],
   "source": [
    "# check tet and train dimensions\n",
    "print(test_all.shape)\n",
    "print(df_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set aside the text corpus\n",
    "description = [literal_eval(s) for s in df_train.description.tolist()]\n",
    "df_train.description = description\n",
    "# PyTorch NN needs a vocab list, or unique vocab terms\n",
    "vocab = []\n",
    "vocab = gensim_model.wv.index_to_key#list(set(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "gather": {
     "logged": 1670362485203
    }
   },
   "outputs": [],
   "source": [
    "# make the train tensors for text to embeddings X and for categories/products Y\n",
    "x_train = [[myembeddings[j] for j in i] for i in description]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "gather": {
     "logged": 1667241596775
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# make the test tensors for text to embeddings X and for categories/products Y\n",
    "test_description = [literal_eval(s) for s in test_all.description.tolist()]\n",
    "\n",
    "x_val = []\n",
    "for i in test_description:\n",
    "    temp = []\n",
    "    for j in i:\n",
    "        # some of the Amazon tokens don't appear in the training data\n",
    "        # so use try/except to skip any unaccounted for Amazon test data tokens\n",
    "        try: temp.append(myembeddings[j])\n",
    "        except: pass\n",
    "    x_val.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"cpu\")\n",
    "#device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19904\n"
     ]
    }
   ],
   "source": [
    "vocab = vocab \n",
    "num_class = 1\n",
    "vocab_size = len(vocab)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = 120"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_xt = []\n",
    "for x in x_train:\n",
    "    if len(x) > maxlen: new_xt.append(np.array(x[:maxlen]))\n",
    "    else: new_xt.append(np.array(list(x) + [0]*(maxlen-len(x)) ) ) #concatenate list with padding zeros and convert to array\n",
    "new_xt = np.array(new_xt)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7613 7613\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the arrays should be the same length\n",
    "print(len(new_xt),  len(x_train))\n",
    "len(new_xt) ==  len(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_xv = []\n",
    "for x in x_val:\n",
    "    if len(x) > maxlen: new_xv.append(np.array(x[:maxlen]))\n",
    "    else: new_xv.append(np.array(list(x) + [0]*(maxlen-len(x)) )  )\n",
    "    \n",
    "new_xv = np.array(new_xv)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = torch.tensor(new_xt, dtype=torch.long)\n",
    "x_cv = torch.tensor(new_xv, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "_uuid": "deee49df5ca1c4413f71677939e26aa1ff784e44",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19904\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(vocab)\n",
    "print(vocab_size)\n",
    "embed_size = 200 # how big is each word vector\n",
    "max_features = vocab_size\n",
    "batch_size = 64 # how many samples to process at once\n",
    "n_splits = 8 # Number of K-fold Splits\n",
    "SEED = 10\n",
    "debug = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "0a78496e4d88d8fb351cdf26d02f1554821ed445"
   },
   "source": [
    "## Pytorch Model - TextCNN\n",
    "based on https://www.kaggle.com/code/mlwhiz/multiclass-text-classification-pytorch\n",
    "but with different loss function loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "so that the results can be multi-label instead of multi-class,\n",
    "meaning that there can be multiple products per narrative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_Text(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_class):\n",
    "        super(CNN_Text, self).__init__()\n",
    "        filter_sizes = [1,2,3,5]\n",
    "        num_filters = 128\n",
    "        #n_classes = len(le.classes_)\n",
    "        n_classes = num_class\n",
    "        self.embedding = nn.Embedding(max_features, embed_size)\n",
    "        #self.embedding.weight = nn.Parameter(torch.tensor(gensim_model.wv.syn0, dtype=torch.float32))\n",
    "        #self.embedding.weight.requires_grad = False\n",
    "        self.convs1 = nn.ModuleList([nn.Conv2d(1, num_filters, (K, embed_size)) for K in filter_sizes])\n",
    "        self.dropout = nn.Dropout(0.3)#nn.Dropout(0.2)\n",
    "        self.fc1 = nn.Linear(len(filter_sizes)*num_filters, n_classes)\n",
    "        self.elu = nn.ELU()\n",
    "        self.silu = nn.SiLU()\n",
    "        self.softplus = nn.Softplus()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)  \n",
    "        x = x.unsqueeze(1)  \n",
    "        #x = [F.relu(conv(x)).squeeze(3) for conv in self.convs1] \n",
    "        x = [F.elu(conv(x)).squeeze(3) for conv in self.convs1] \n",
    "        x = [F.max_pool1d(i, i.size(2)).squeeze(2) for i in x]  \n",
    "        x = torch.cat(x, 1)\n",
    "        x = self.dropout(x)  \n",
    "        logit = self.fc1(x)\n",
    "\n",
    "        return logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTM(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_class):\n",
    "        super(BiLSTM, self).__init__()\n",
    "        self.hidden_size = 128\n",
    "        drp = 0.4\n",
    "        n_classes = num_class#len(le.classes_)\n",
    "        self.embedding = nn.Embedding(max_features, embed_size)\n",
    "        #self.embedding.weight = nn.Parameter(torch.tensor(embedding_matrix, dtype=torch.float32))\n",
    "        #self.embedding.weight.requires_grad = False\n",
    "        #self.embedding.weight = nn.Parameter(torch.tensor(gensim_model.wv.syn0, dtype=torch.float32))\n",
    "        #self.embedding.weight.requires_grad = False\n",
    "        self.lstm = nn.LSTM(embed_size, self.hidden_size, bidirectional=True, batch_first=True)\n",
    "        self.linear = nn.Linear(4*self.hidden_size, 6*self.hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(drp)\n",
    "        self.linear2 = nn.Linear(6*self.hidden_size, 3*self.hidden_size)\n",
    "        self.out = nn.Linear(3*self.hidden_size, n_classes)\n",
    "        \n",
    "        self.elu = nn.ELU()\n",
    "        self.silu = nn.SiLU()\n",
    "        self.softplus = nn.Softplus()\n",
    "\n",
    "    def forward(self, x):\n",
    "        #rint(x.size())\n",
    "        h_embedding = self.embedding(x)\n",
    "        h_embedding = self.elu(h_embedding)\n",
    "        #_embedding = torch.squeeze(torch.unsqueeze(h_embedding, 0))\n",
    "        h_lstm, _ = self.lstm(h_embedding)\n",
    "        avg_pool = torch.mean(h_lstm, 1)\n",
    "        max_pool, _ = torch.max(h_lstm, 1)\n",
    "        conc = torch.cat(( avg_pool, max_pool), 1)\n",
    "        \n",
    "        conc = self.relu(self.linear(conc))\n",
    "        conc = self.dropout(conc)\n",
    "        conc = self.softplus(self.linear2(conc))# extra hidden layer\n",
    "        conc = self.dropout(conc)\n",
    "        out = self.out(conc)\n",
    "#        out = self.softplus(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch Model - BiLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN_Text(\n",
      "  (embedding): Embedding(19904, 200)\n",
      "  (convs1): ModuleList(\n",
      "    (0): Conv2d(1, 128, kernel_size=(1, 200), stride=(1, 1))\n",
      "    (1): Conv2d(1, 128, kernel_size=(2, 200), stride=(1, 1))\n",
      "    (2): Conv2d(1, 128, kernel_size=(3, 200), stride=(1, 1))\n",
      "    (3): Conv2d(1, 128, kernel_size=(5, 200), stride=(1, 1))\n",
      "  )\n",
      "  (dropout): Dropout(p=0.3, inplace=False)\n",
      "  (fc1): Linear(in_features=512, out_features=1, bias=True)\n",
      "  (elu): ELU(alpha=1.0)\n",
      "  (silu): SiLU()\n",
      "  (softplus): Softplus(beta=1, threshold=20)\n",
      ")\n",
      "BiLSTM(\n",
      "  (embedding): Embedding(19904, 200)\n",
      "  (lstm): LSTM(200, 128, batch_first=True, bidirectional=True)\n",
      "  (linear): Linear(in_features=512, out_features=768, bias=True)\n",
      "  (relu): ReLU()\n",
      "  (dropout): Dropout(p=0.4, inplace=False)\n",
      "  (linear2): Linear(in_features=768, out_features=384, bias=True)\n",
      "  (out): Linear(in_features=384, out_features=1, bias=True)\n",
      "  (elu): ELU(alpha=1.0)\n",
      "  (silu): SiLU()\n",
      "  (softplus): Softplus(beta=1, threshold=20)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(CNN_Text(num_class))\n",
    "print(BiLSTM(num_class))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "0da30e2afce23b753796f3045b44ce91a07e4303",
    "tags": []
   },
   "source": [
    "## Train NN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model_type='cnn'):\n",
    "\n",
    "    num_class = 2# number of classes is the highest number class plus 1 because class keys start at 0\n",
    "    print(num_class,'labels')\n",
    "    \n",
    "    new_yt = []# train\n",
    "    for y in df_train.target.tolist():\n",
    "        temp = np.zeros(num_class)# instantiate output nodes for that entry\n",
    "        temp[y] = 1# look up the category in the dictionary and use that as the index set to positive (1) in the output array\n",
    "        new_yt.append(temp)#append output nodes to the ylist\n",
    "\n",
    "    new_yv = []# test / val\n",
    "    for y in test_all.target.tolist():\n",
    "        temp = np.zeros(num_class)# instantiate output nodes for that entry\n",
    "        temp[y] = 1# look up the category in the dictionary and use that as the index set to positive (1) in the output array\n",
    "        new_yv.append(temp)#append output nodes to the ylist\n",
    "    \n",
    "    y_train = torch.tensor(new_yt)#[torch.tensor(i).cuda() for i in new_yt]#, dtype=torch.long).cuda()\n",
    "    y_cv = torch.tensor(new_yv)\n",
    "    #print(new_yt[0], new_yv[0])\n",
    "    # Create Torch datasets\n",
    "    train = torch.utils.data.TensorDataset(x_train, y_train)\n",
    "    valid = torch.utils.data.TensorDataset(x_cv, y_cv)\n",
    "    \n",
    "    batch_size = 128\n",
    "\n",
    "    # use loaders to iterate through data batches during training\n",
    "    train_loader = torch.utils.data.DataLoader(train, batch_size=batch_size, shuffle=True)\n",
    "    valid_loader = torch.utils.data.DataLoader(valid, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    n_epochs = 2# number of epochs\n",
    "    # one function that can do either CNN or LSTM\n",
    "    if 'cnn' in model_type.lower(): \n",
    "        model = CNN_Text(num_class)\n",
    "        gamma_val = 0.05# low learning rate for CNN\n",
    "    elif 'lstm' in model_type.lower(): \n",
    "        model = BiLSTM(num_class)\n",
    "        gamma_val = 0.3# higher learning rate for LSTM\n",
    "    \n",
    "    model\n",
    "    loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "    optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=0.02)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1, gamma=gamma_val)# gamma val goes here, what the LR is multiplied by per iteration by the scheduler\n",
    "\n",
    "    train_loss = []\n",
    "    valid_loss = []\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        start_time = time.time()\n",
    "        # Set model to train configuration\n",
    "        model.train()\n",
    "        avg_loss = 0.  \n",
    "        for i, (x_batch, y_batch) in enumerate(train_loader):\n",
    "            # Predict/Forward Pass\n",
    "            y_pred = model(x_batch)\n",
    "            # Compute loss\n",
    "            loss = loss_fn(y_pred, y_batch)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            avg_loss += loss.item() / len(train_loader)\n",
    "            #print(loss)\n",
    "\n",
    "        # Set model to validation configuration -Doesn't get trained here\n",
    "        model.eval()        \n",
    "        avg_val_loss = 0.\n",
    "        #val_preds = np.zeros((len(x_cv),len(le.classes_)))\n",
    "        val_preds = np.zeros((len(x_cv),num_class))\n",
    "\n",
    "        for i, (x_batch, y_batch) in enumerate(valid_loader):\n",
    "            y_pred = model(x_batch).detach()\n",
    "            avg_val_loss += loss_fn(y_pred, y_batch).item() / len(valid_loader)\n",
    "            # keep/store predictions\n",
    "            val_preds[i * batch_size:(i+1) * batch_size] = F.softmax(y_pred).cpu().numpy()#y_pred.cpu().numpy()#F.softmax(y_pred).cpu().numpy()\n",
    "            #val_preds[i * batch_size:(i+1) * batch_size] = F.sigmoid(y_pred).cpu().numpy()#y_pred.cpu().numpy()#F.softmax(y_pred).cpu().numpy()\n",
    "\n",
    "    # Check Accuracy\n",
    "        pred_ = [np.where(i>0.1) for i in val_preds]#[np.where(i>0) for i in val_preds]\n",
    "        val_ = [np.where(i==1) for i in new_yv]\n",
    "        # tps are elements j of predicted lists that are in the val lists, list by list, i,k,\n",
    "        tps = 0\n",
    "        fps = 0\n",
    "        fns = 0\n",
    "        for i,k in zip(pred_, val_):\n",
    "            if len(i[0]) > 0:\n",
    "                for j in i:\n",
    "                    if j[0] in list(k[0]): tps+=1\n",
    "                    elif j[0] not in list(k[0]): fps+=1\n",
    "            if len(k[0]) > 0:\n",
    "                for m in k:\n",
    "                    if m[0] not in list(i[0]): fns+=1\n",
    "\n",
    "        try: precision = tps/(tps+fps)\n",
    "        except: precision = 0\n",
    "        try: recall = tps/(tps+fns)\n",
    "        except: recall = 0\n",
    "        try: f1 = (2 * precision * recall) / (precision + recall)\n",
    "        except: f1 = 0\n",
    "        print(round(precision,4), 'precision.', round(recall,4), 'recall.', round(f1,4), 'F1.')\n",
    "        val_accuracy = f1#sum(val_preds.argmax(axis=1)==new_yv)/len(new_yv)\n",
    "        train_loss.append(avg_loss)\n",
    "        valid_loss.append(avg_val_loss)\n",
    "        elapsed_time = time.time() - start_time \n",
    "        print('Epoch {}/{} \\t loss={:.4f} \\t val_loss={:.4f}  \\t val_acc={:.4f}  \\t time={:.2f}s'.format(\n",
    "                    epoch + 1, n_epochs, avg_loss, avg_val_loss, val_accuracy, elapsed_time))\n",
    "\n",
    "        scheduler.step()\n",
    "    torch.save(model,'{}_model'.format(model_type))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 labels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Owner\\AppData\\Local\\Temp\\ipykernel_18476\\1857778842.py:18: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  C:\\cb\\pytorch_1000000000000\\work\\torch\\csrc\\utils\\tensor_new.cpp:204.)\n",
      "  y_train = torch.tensor(new_yt)#[torch.tensor(i).cuda() for i in new_yt]#, dtype=torch.long).cuda()\n",
      "C:\\Users\\Owner\\AppData\\Local\\Temp\\ipykernel_18476\\1857778842.py:74: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  val_preds[i * batch_size:(i+1) * batch_size] = F.softmax(y_pred).cpu().numpy()#y_pred.cpu().numpy()#F.softmax(y_pred).cpu().numpy()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.895 precision. 0.9624 recall. 0.9275 F1.\n",
      "Epoch 1/2 \t loss=1.4437 \t val_loss=0.2718  \t val_acc=0.9275  \t time=68.31s\n",
      "0.92 precision. 0.9634 recall. 0.9412 F1.\n",
      "Epoch 2/2 \t loss=0.2461 \t val_loss=0.1622  \t val_acc=0.9412  \t time=68.55s\n",
      "this loop took 0.04 hours to run\n"
     ]
    }
   ],
   "source": [
    "# this is the generic category level and the cat_selection variable is just a dummy variable\n",
    "start_time = time.time()\n",
    "train_model(model_type='cnn')\n",
    "duration = round((time.time() - start_time)/60/60, 2)# duration in seconds, round to reasonable decimal\n",
    "print(\"this loop took {} hours to run\".format(duration))# print time it took, seconds converted to minutes to hours\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('{}_model'.format(\"cnn\"), map_location=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "gather": {
     "logged": 1667241596775
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# make the test tensors for text to embeddings X and for categories/products Y\n",
    "test_data = pd.read_csv('test_all_processed.csv')\n",
    "test_description = [literal_eval(s) for s in test_data.description.tolist()]\n",
    "\n",
    "x_val = []\n",
    "for i in test_description:\n",
    "    temp = []\n",
    "    for j in i:\n",
    "        # some of the Amazon tokens don't appear in the training data\n",
    "        # so use try/except to skip any unaccounted for Amazon test data tokens\n",
    "        try: temp.append(myembeddings[j])\n",
    "        except: pass\n",
    "    x_val.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_xv = []\n",
    "for x in x_val:\n",
    "    if len(x) > maxlen: new_xv.append(np.array(x[:maxlen]))\n",
    "    else: new_xv.append(np.array(list(x) + [0]*(maxlen-len(x)) )  )\n",
    "    \n",
    "new_xv = np.array(new_xv)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Owner\\AppData\\Local\\Temp\\ipykernel_18476\\2977559241.py:3: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  val_preds = F.softmax(y_pred).cpu().numpy()#y_pred.cpu().numpy()#F.softmax(y_pred).cpu().numpy()\n"
     ]
    }
   ],
   "source": [
    "x_cv = torch.tensor(new_xv, dtype=torch.long)\n",
    "y_pred = model(x_cv).detach()\n",
    "val_preds = F.softmax(y_pred).cpu().numpy()#y_pred.cpu().numpy()#F.softmax(y_pred).cpu().numpy()\n",
    "# Check Accuracy\n",
    "pred_ = [np.where(i>0.5) for i in val_preds]#[np.where(i>0) for i in val_preds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " ...]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_ = [np.where(i>0.5)[0][0] for i in val_preds]#[np.where(i>0) for i in val_preds]\n",
    "pred_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1166\n"
     ]
    }
   ],
   "source": [
    "print(sum(pred_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({'id':test_data.id.tolist(), 'target':pred_}).to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    }
   ],
   "source": [
    "print('Done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### "
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
